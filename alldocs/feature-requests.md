# Feature Requests - Run through this when the engine is complete v0.1

* Add check to prevent uploads until most recent upload is complete, otherwise you get stuck loading bar and that's no fun.

* Add and clean the drag / drop feature.

* Support multiple users. More server storage above `app_data = {}`.

* Conversation with Agent to run subselects, ie it returns a query result, and
can use that returned result with a new user query to join on it's own columns.

* So to make customers happy would be to abstract the pydantic classes out so
I could easily form them differently based on my customer's data... that I could see
as having generalized value across multiple customers... a data dictionary won't be enough
because the classes for pydantic would need to be hard coded for garunteed high quality llm results. But this is where the AI app specialization or "niche" value comes in here

* Utilize the user's metadata to enhance the user's query and extract api
parameters from the combined data. This should be smart, and should
allow for more finely tuned, improved api call results. This in turn provides
better data in gcs for bigquery predictive analytics on all combined retrieved data.

* They could even ask queries like, show me the columns generated by the openmeteo
api that relate to my data?
Answering that question would be valuable, then they could ask a more focused follow up
query to get value from the enrichment source

* Add dynamic job_id so with multiple users, the llm can retrieve the query and schema
specific to the user's data ???
Import for two key reason among others:

1. Provide a correlation key. The jobId is still the only way for your server to know that the schema it received from the Cloud Function is the result of the most recent ETL process, rather than a delayed result from a previous one
2. Ensure a clean state. The unique jobId guarantees that when the final Cloud Function deletes the Firestore document, it is removing a specific, completed job, not accidentally interfering with a new one that might start while the previous one is still finishing.
3. Create a unique document in Firestore. Each time the user runs a query, a new ETL process is initiated. The jobId is the key that ensures a fresh, clean document is created in Firestore to track this specific ETL run.

* PREVENT LLM INJECTION: We took care of the biggest security concern by removing the
javascript logic and transitioning it to server side callback, we will worry about the llm injection handles later, TBD integrated with the cloud function.
No llms on server, no validation checks on server, do it in the cloud.

* Script to delete all bucket, firestore, bigquery data